# ğŸŒ¦ï¸ Projeto de Pipeline ClimÃ¡tico com Kafka, Spark e PostgreSQL



Este projeto demonstra como criar um pipeline de dados em tempo real utilizando **Apache Kafka**, **Apache Spark**, **PostgreSQL** e **Docker**, consumindo dados reais da API do OpenWeatherMap sobre o clima em **SÃ£o Paulo**. O objetivo Ã© simular uma arquitetura robusta de ingestÃ£o e processamento de eventos meteorolÃ³gicos.

<p align="center">
  <img src="images/tools.png">
</p>


## ğŸ”§ Tecnologias Utilizadas

- [Python 3.10+](https://www.python.org/)
- [Apache Kafka](https://kafka.apache.org/)
- [Apache Spark](https://spark.apache.org/)
- [PostgreSQL 13+](https://www.postgresql.org/)
- [Docker + Docker Compose](https://www.docker.com/)
- Bibliotecas Python:
  - `kafka-python`
  - `requests`
  - `psycopg2-binary`
  - `pyspark`

---

## ğŸ“Œ Arquitetura do Projeto

<h3 align="center">ğŸ“Š Arquitetura do Pipeline</h3>

<p align="center">
  <img src="images/arquitetura.png" alt="Diagrama da Arquitetura" width="80%">
</p>

---

## ğŸš€ Como Executar o Projeto

### âœ… Forma RÃ¡pida (Windows)

VocÃª pode iniciar todo o projeto diretamente com o script:

```bash
init.bat
```

Esse script realiza os seguintes passos:
- Ativa o ambiente virtual `.venv`
- Instala as dependÃªncias necessÃ¡rias
- Sobe os containers com Docker
- Executa o `producer.py`
- Abre um terminal Ã  parte para vocÃª rodar o Spark Consumer

---

### 1. Clonar o repositÃ³rio

```bash
git clone https://github.com/seu-usuario/projeto-climatico.git
cd projeto-climatico
```

### 2. Subir os serviÃ§os com Docker

```bash
docker-compose up -d
```

Este comando irÃ¡ iniciar os seguintes containers:
- Apache Zookeeper (porta 2181)
- Apache Kafka Broker (porta 9092)
- PostgreSQL com o banco `weather_db` e a tabela `weather_events` jÃ¡ criados via script SQL.

---

### 3. Criar ambiente virtual e instalar dependÃªncias

```bash
python -m venv .venv
.\.venv\Scripts\activate  # Windows
# ou
source .venv/bin/activate  # Linux/Mac
```

Instalar as dependÃªncias:

```bash
pip install -r requirements.txt
pip install -r spark_requirements.txt
```

---

### 4. Rodar o Kafka Producer (enviando dados da API)

```bash
python producer.py
```

Este script envia dados climÃ¡ticos reais de SÃ£o Paulo a cada 5 minutos para o tÃ³pico Kafka `weather`.

---

### 5. Rodar o Spark Structured Streaming Consumer

```bash
spark-submit spark_consumer.py
```

Esse script:
- LÃª os dados do tÃ³pico `weather` do Kafka
- Converte os JSONs para DataFrame estruturado
- Escreve no PostgreSQL em modo `append`, com `created_at`

---

## ğŸ“Š Estrutura da Tabela PostgreSQL

A tabela `weather_events` Ã© criada automaticamente ao subir o container. Estrutura:

| Coluna          | Tipo              |
|-----------------|-------------------|
| station_name    | VARCHAR            |
| event_timestamp | TIMESTAMP          |
| temperature     | DOUBLE PRECISION   |
| humidity        | DOUBLE PRECISION   |
| precipitation   | DOUBLE PRECISION   |
| wind_speed      | DOUBLE PRECISION   |
| created_at      | TIMESTAMP          |

---

## ğŸ“ Estrutura de Pastas

```bash
projeto-climatico/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ create_tables.sql
â”œâ”€â”€ producer.py
â”œâ”€â”€ spark_consumer.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ spark_requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ init.bat
â””â”€â”€ README.md
```

---

## ğŸ§ª Exemplo de Evento Kafka

```json
{
  "station_name": "Sao Paulo",
  "event_timestamp": "2025-05-04T12:00:00-03:00",
  "temperature": 24.5,
  "humidity": 78,
  "precipitation": 0.0,
  "wind_speed": 3.1
}
```

---

## ğŸ’¡ Possiveis Melhorias

- Dockerizar o Spark e o Producer
- Criar visualizaÃ§Ã£o interativa com Streamlit ou Grafana
- Adicionar camada de monitoramento e alertas
- Expandir para mÃºltiplas cidades ou sensores simulados

---
