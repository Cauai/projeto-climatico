# Projeto de Streaming de Dados ClimÃ¡ticos ðŸŒ¦ï¸

Este projeto simula o envio de dados meteorolÃ³gicos em tempo real usando Apache Kafka, Python e PostgreSQL, tudo orquestrado com Docker Compose.

---

## ðŸ› ï¸ Tecnologias utilizadas

- **Python 3.10+**
- **Apache Kafka**
- **PostgreSQL**
- **Docker + Docker Compose**
- **Kafka-Python** (producer e consumer)
- **psycopg2-binary** (conexÃ£o PostgreSQL)

---

## ðŸš€ Como executar o projeto

### 1. Clonar o repositÃ³rio

```bash
git clone https://github.com/seu-usuario/projeto-climatico.git
cd projeto-climatico
```

### 2. Subir os serviÃ§os com Docker Compose

```bash
docker-compose up -d
```

Este comando irÃ¡ iniciar:
- Zookeeper
- Kafka Broker
- PostgreSQL (Banco `weather_db`)

O PostgreSQL jÃ¡ serÃ¡ inicializado com a tabela `weather_events`.

---

### 3. Criar e ativar o ambiente virtual no VSCode

```bash
# Criar o ambiente
python -m venv .venv

# Ativar no Windows PowerShell
.\.venv\Scripts\Activate.ps1
```

VocÃª verÃ¡ seu terminal assim:
```
(.venv) C:\Users\SeuUsuario\projeto-climatico>
```

---

### 4. Instalar as dependÃªncias Python

```bash
pip install kafka-python psycopg2-binary
```

Opcionalmente, gerar o arquivo de dependÃªncias:

```bash
pip freeze > requirements.txt
```

---

### 5. Rodar o Producer (envio de dados climÃ¡ticos para o Kafka)

```bash
python producer/producer.py
```

O `producer.py` irÃ¡:
- Gerar dados sintÃ©ticos de clima
- Enviar eventos para o tÃ³pico Kafka chamado `weather` a cada 5 segundos

Exemplo de mensagem enviada:
```json
{
  "station_name": "Estacao_Sao_Paulo",
  "event_timestamp": "2025-04-28T21:00:12.345678",
  "temperature": 26.5,
  "humidity": 70.0,
  "precipitation": 0.0,
  "wind_speed": 15.2
}
```

---

### 6. Rodar o Consumer (opcional - em construÃ§Ã£o)

O consumer irÃ¡:
- Ler os eventos do Kafka
- Inserir os dados automaticamente no banco `weather_db` na tabela `weather_events`

(Em breve...)

---

## ðŸ“‚ Estrutura do projeto

```bash
projeto-climatico/
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ database/
â”‚   â””â”€â”€ create_tables.sql
â”œâ”€â”€ producer/
â”‚   â””â”€â”€ producer.py
â”œâ”€â”€ consumer/          # (a ser criado)
â”‚   â””â”€â”€ consumer.py
â”œâ”€â”€ .venv/             # Ambiente virtual
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ðŸ“‹ ObservaÃ§Ãµes

- A tabela `weather_events` Ã© criada automaticamente na primeira subida do container PostgreSQL.
- Certifique-se de que o ambiente virtual `.venv` esteja ativado antes de rodar os scripts Python.
- No DBeaver, desative o SSL para conectar ao banco PostgreSQL local.
- Kafka e Zookeeper devem estar rodando para que o Producer e Consumer funcionem corretamente.

---

## ðŸ”¥ PrÃ³ximos passos

- Criar o `consumer.py` para integrar o Kafka ao PostgreSQL.
- Implementar monitoramento bÃ¡sico dos fluxos.
- (Opcional) Dockerizar a aplicaÃ§Ã£o Python para rodar Producer/Consumer em containers tambÃ©m.

---