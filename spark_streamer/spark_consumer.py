from pyspark.sql import SparkSession # type: ignore
from pyspark.sql.functions import from_json, col # type: ignore
from pyspark.sql.types import StructType, StringType, DoubleType # type: ignore
from datetime import datetime
# 1. Cria√ß√£o da SparkSession (ponto de entrada do Spark)
spark = (
    SparkSession.builder
    .appName("KafkaSparkWeatherConsumer")  # Nome da aplica√ß√£o no Spark UI
    .master("local[*]")                    # Usa todos os n√∫cleos da m√°quina local
    .getOrCreate()
)

# Reduz a verbosidade dos logs
spark.sparkContext.setLogLevel("WARN")

# 2. Schema esperado do JSON vindo do Kafka
schema = (
    StructType()
    .add("station_name", StringType())       # Nome da esta√ß√£o (ex: S√£o Paulo)
    .add("event_timestamp", StringType())    # Data e hora do evento
    .add("temperature", DoubleType())        # Temperatura em Celsius
    .add("humidity", DoubleType())           # Umidade relativa (%)
    .add("wind_speed", DoubleType())         # Velocidade do vento (m/s)
    .add("precipitation",DoubleType())
)

# 3. L√™ os dados do t√≥pico 'weather' no Kafka
df_kafka_raw = (
    spark.readStream
    .format("kafka")
    .option("kafka.bootstrap.servers", "localhost:9092")  # Endere√ßo do broker
    .option("subscribe", "weather")                       # Nome do t√≥pico
    .option("startingOffsets", "latest")                  # Come√ßa a ler os dados novos
    .load()
)

# 4. Converte o valor bin√°rio (Kafka) para JSON estruturado
df_parsed = (
    df_kafka_raw
    .selectExpr("CAST(value AS STRING) AS json_str")                 # Converte para string
    .select(from_json(col("json_str"), schema).alias("data"))        # Aplica o schema
    .select("data.*")                                                # Expande as colunas
)


# 5. Fun√ß√£o para salvar no PostgreSQL com log e timestamp
def save_to_postgres(batch_df, batch_id):
    # Adiciona coluna de salvamento
    batch_df = batch_df.withColumn("created_at", current_timestamp())

    # Log
    print(f"\nüü¢ [Batch {batch_id}] {datetime.now()} - Salvando {batch_df.count()} registro(s) no PostgreSQL")
    batch_df.select("station_name", "event_timestamp", "created_at").show(truncate=False)

    # Escrita no banco
    batch_df.write \
        .format("jdbc") \
        .option("url", "jdbc:postgresql://localhost:5432/weather_db") \
        .option("dbtable", "weather_events") \
        .option("user", "postgres") \
        .option("password", "postgres") \
        .option("driver", "org.postgresql.Driver") \
        .mode("append") \
        .save()

# 5. Mostra os dados em tempo real no console
query = df_parsed.writeStream \
    .format("console") \
    .outputMode("append") \
    .trigger(processingTime="1 minute") \
    .start()

# 6. Mant√©m a aplica√ß√£o rodando
query.awaitTermination()
